{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "yelp-text-processing.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "QNpmtfJnHHIG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -q langdetect\n",
        "!pip install autocorrect\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import nltk\n",
        "from nltk import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize\n",
        "import time\n",
        "import warnings\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "from nltk.stem import PorterStemmer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.svm import LinearSVC\n",
        "from autocorrect import Speller\n",
        "import string\n",
        "from bs4 import BeautifulSoup\n",
        "from langdetect import detect\n",
        "from textblob import TextBlob\n",
        "\n",
        "porter = PorterStemmer()\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "warnings.filterwarnings(action = 'ignore') "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ieyt1Fd5HrUt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "las_vegas_data= pd.read_pickle('/content/drive/My Drive/las_vegas_data.pkl')\n",
        "las_vegas_data.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvlHnaVaHs4K",
        "colab_type": "code",
        "outputId": "b93d9b13-f9e1-422d-af7c-52ff1b67a322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "source": [
        "start= time.time()\n",
        "las_vegas_data['length'] = las_vegas_data['text'].apply(len)\n",
        "las_vegas_data=las_vegas_data[las_vegas_data['length']>=50]\n",
        "las_vegas_data['language']=las_vegas_data['text'].apply(detect)\n",
        "las_vegas_data=las_vegas_data[las_vegas_data['language']== 'en']\n",
        "print(\"Time to load data: {} seconds\".format(time.time() - start))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to load data: 8.343015909194946 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "je6jEgRB-iZB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "las_vegas_data.to_pickle('/content/drive/My Drive/las_vegas_data_only_english.pkl')\n",
        "las_vegas_data=pd.read_pickle('/content/drive/My Drive/las_vegas_data_only_english.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVTqICF1cOGD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def filter_punctuation(text):\n",
        "  filtered= \"\".join([c for c in text if c not in string.punctuation])\n",
        "  return filtered\n",
        "def filter_html(text):\n",
        "  soup=BeautifulSoup(text, 'lxml')\n",
        "  filtered= soup.get_text()\n",
        "  return filtered\n",
        "def filter_stopwords(text):\n",
        "  stop_words = set(stopwords.words('english'))\n",
        "  filtered= [item for item in text if item not in stop_words]\n",
        "  return filtered\n",
        "def lemmatize(text):\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  lemmatized= [lemmatizer.lemmatize(i) for i in text]\n",
        "  return lemmatized\n",
        "\n",
        "def stemmize(text):\n",
        "  stemmer = PorterStemmer()\n",
        "  stemmed= \" \".join([stemmer.stem(i) for i in text])\n",
        "  return stemmed"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kZFerFA9LiED",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "start= time.time()\n",
        "las_vegas_data.text = las_vegas_data.text.str.lower()\n",
        "las_vegas_data['text']= las_vegas_data.text.str.replace('[!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~]','')\n",
        "las_vegas_data['text']=las_vegas_data['text'].apply(lambda x: filter_punctuation(x))\n",
        "las_vegas_data['text']=las_vegas_data['text'].apply(lambda x: filter_html(x))\n",
        "las_vegas_data['text']=las_vegas_data['text'].apply(lambda x: lower(x))\n",
        "las_vegas_data['tokenized_text'] = las_vegas_data['text'].apply(sent_tokenize)\n",
        "las_vegas_data['tokenized_text'] = las_vegas_data['text'].apply(word_tokenize) \n",
        "las_vegas_data['tokenized_text'] = las_vegas_data['tokenized_text'].apply(lambda tokens: [word for word in tokens if word.isalpha()])        \n",
        "las_vegas_data['tokenized_text']=las_vegas_data['tokenized_text'].apply(lambda x: filter_stopwords(x))\n",
        "las_vegas_data['tokenized_text']=las_vegas_data['tokenized_text'].apply(lambda x: lemmatize(x) )\n",
        "# las_vegas_data['tokenized_text']=las_vegas_data['tokenized_text'].apply(lambda x: stemmer(x) )\n",
        "print(\"Time to load data: {} seconds\".format(time.time() - start))\n",
        "las_vegas_data"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52j8tLVBVaP5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# spell= Speller(lang='en')\n",
        "# las_vegas_data['text']=las_vegas_data['text'].apply(spell)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7R9KyZ9JtZ-0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "fd79c2ab-70bf-45c1-8562-82d5312f471d"
      },
      "source": [
        "start= time.time()\n",
        "las_vegas_data['label'] = ''\n",
        "las_vegas_data.loc[las_vegas_data.stars >=3, 'label'] = 'positive'\n",
        "las_vegas_data.loc[las_vegas_data.stars <3, 'label'] = 'negative'\n",
        "# las_vegas_data.tokenized_text = las_vegas_data.tokenized_text.str.lower()\n",
        "print(\"Time to load data: {} seconds\".format(time.time() - start))"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time to load data: 0.3048112392425537 seconds\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BesPn3uGiXiS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "las_vegas_data.drop(columns=['length'], inplace=True)\n",
        "las_vegas_data.drop(columns=['language'], inplace=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tx9SLa9HiHI4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "las_vegas_data['polarity'] = las_vegas_data['text'].apply(lambda text: TextBlob(text).sentiment.polarity) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHt7cPGWjQDd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "las_vegas_data.to_pickle('/content/drive/My Drive/las_vegas_data_tokenized.pkl')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhJeK19piLoq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 728
        },
        "outputId": "8669eb9a-a4ae-4304-c725-21004d79975b"
      },
      "source": [
        "las_vegas_data"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>business_id</th>\n",
              "      <th>review_id</th>\n",
              "      <th>stars</th>\n",
              "      <th>text</th>\n",
              "      <th>tokenized_text</th>\n",
              "      <th>label</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>tstimHoMcYbkSC4eBA1wEg</td>\n",
              "      <td>iWFBGYotfzwiLsOka0e1Rw</td>\n",
              "      <td>4.5</td>\n",
              "      <td>we found out about this gem from the mans cowo...</td>\n",
              "      <td>[found, gem, man, coworker, used, live, apt, c...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.238242</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>tstimHoMcYbkSC4eBA1wEg</td>\n",
              "      <td>QhiVo4F8MFSoz1T7YggcXA</td>\n",
              "      <td>4.5</td>\n",
              "      <td>the chips are a fried corn tortilla which were...</td>\n",
              "      <td>[chip, fried, corn, tortilla, surprisingly, am...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.342857</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>tstimHoMcYbkSC4eBA1wEg</td>\n",
              "      <td>FUv-c5CkLy71yFTGLrp2ag</td>\n",
              "      <td>4.5</td>\n",
              "      <td>the pollo mole is well worth the trip out here...</td>\n",
              "      <td>[pollo, mole, well, worth, trip, service, frie...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.377083</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tstimHoMcYbkSC4eBA1wEg</td>\n",
              "      <td>lJfGtkCc4in6Xcq-_Wtfwg</td>\n",
              "      <td>4.5</td>\n",
              "      <td>ive ordered caldo de rez from here twice and b...</td>\n",
              "      <td>[ive, ordered, caldo, de, rez, twice, time, fo...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.109524</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>tstimHoMcYbkSC4eBA1wEg</td>\n",
              "      <td>BKoH9aG1ZjUjy2brzkxzJw</td>\n",
              "      <td>4.5</td>\n",
              "      <td>my boyfriend doesnt like mexican food yet he l...</td>\n",
              "      <td>[boyfriend, doesnt, like, mexican, food, yet, ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.245631</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>712589</th>\n",
              "      <td>p5rpYtxS5xPQjt3MXYVEwA</td>\n",
              "      <td>M5Z6ju1IxgXyHbuTB4b0Ew</td>\n",
              "      <td>4.0</td>\n",
              "      <td>weak to say the least nobody knows whats going...</td>\n",
              "      <td>[weak, say, least, nobody, know, whats, going,...</td>\n",
              "      <td>positive</td>\n",
              "      <td>-0.229167</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>712590</th>\n",
              "      <td>p5rpYtxS5xPQjt3MXYVEwA</td>\n",
              "      <td>pn5TS8GiEs0iE_O1dTkWuQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>i ordered the chicken quinoa salad i cant beli...</td>\n",
              "      <td>[ordered, chicken, quinoa, salad, cant, believ...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.257778</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>712591</th>\n",
              "      <td>p5rpYtxS5xPQjt3MXYVEwA</td>\n",
              "      <td>ACUg1ScLb6pwL6jJyp65-w</td>\n",
              "      <td>4.0</td>\n",
              "      <td>i love this place salmon with steamed asparagu...</td>\n",
              "      <td>[love, place, salmon, steamed, asparagus, brow...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.471875</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>712592</th>\n",
              "      <td>p5rpYtxS5xPQjt3MXYVEwA</td>\n",
              "      <td>_51tDhnA8WJY8VmUg1Cu6w</td>\n",
              "      <td>4.0</td>\n",
              "      <td>this is my go to spot when im too lazy to find...</td>\n",
              "      <td>[go, spot, im, lazy, find, food, food, smoothy...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.173333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>712593</th>\n",
              "      <td>p5rpYtxS5xPQjt3MXYVEwA</td>\n",
              "      <td>PUKKFmFVY7WScEh4zHpIRQ</td>\n",
              "      <td>4.0</td>\n",
              "      <td>greens and proteins on eastern ave is a great ...</td>\n",
              "      <td>[green, protein, eastern, ave, great, place, g...</td>\n",
              "      <td>positive</td>\n",
              "      <td>0.459737</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>709847 rows × 7 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                   business_id               review_id  ...     label  polarity\n",
              "0       tstimHoMcYbkSC4eBA1wEg  iWFBGYotfzwiLsOka0e1Rw  ...  positive  0.238242\n",
              "1       tstimHoMcYbkSC4eBA1wEg  QhiVo4F8MFSoz1T7YggcXA  ...  positive  0.342857\n",
              "2       tstimHoMcYbkSC4eBA1wEg  FUv-c5CkLy71yFTGLrp2ag  ...  positive  0.377083\n",
              "3       tstimHoMcYbkSC4eBA1wEg  lJfGtkCc4in6Xcq-_Wtfwg  ...  positive  0.109524\n",
              "4       tstimHoMcYbkSC4eBA1wEg  BKoH9aG1ZjUjy2brzkxzJw  ...  positive  0.245631\n",
              "...                        ...                     ...  ...       ...       ...\n",
              "712589  p5rpYtxS5xPQjt3MXYVEwA  M5Z6ju1IxgXyHbuTB4b0Ew  ...  positive -0.229167\n",
              "712590  p5rpYtxS5xPQjt3MXYVEwA  pn5TS8GiEs0iE_O1dTkWuQ  ...  positive  0.257778\n",
              "712591  p5rpYtxS5xPQjt3MXYVEwA  ACUg1ScLb6pwL6jJyp65-w  ...  positive  0.471875\n",
              "712592  p5rpYtxS5xPQjt3MXYVEwA  _51tDhnA8WJY8VmUg1Cu6w  ...  positive  0.173333\n",
              "712593  p5rpYtxS5xPQjt3MXYVEwA  PUKKFmFVY7WScEh4zHpIRQ  ...  positive  0.459737\n",
              "\n",
              "[709847 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYyMtwmllFAL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4r1C9C1TmzlV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t7iQAaqpmzfZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ip6KPXDmmzXt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0HMbITxlE7b",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3gzDTr9hB_a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# start= time.time()\n",
        "\n",
        "# word_list=[\"0o\", \"0s\", \"3a\", \"3b\", \"3d\", \"6b\", \"6o\", \"a\", \"a1\", \"a2\", \"a3\", \"a4\", \"ab\", \"able\", \"about\", \"above\",\n",
        "# \"abst\", \"ac\", \"accordance\", \"according\", \"accordingly\", \"across\", \"act\", \"actually\", \"ad\", \"added\", \"adj\", \"ae\", \"af\",\n",
        "# \"affected\", \"affecting\", \"affects\", \"after\", \"afterwards\", \"ag\", \"again\", \"against\", \"ah\", \"ain\", \"ain't\", \"aj\", \"al\",\n",
        "# \"all\", \"allow\", \"allows\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\", \"am\", \"among\", \"amongst\",\n",
        "# \"amoungst\", \"amount\", \"an\", \"and\", \"announce\", \"another\", \"any\", \"anybody\", \"anyhow\", \"anymore\", \"anyone\", \"anything\",\n",
        "# \"anyway\", \"anyways\", \"anywhere\", \"ao\", \"ap\", \"apart\", \"apparently\", \"appear\", \"appreciate\", \"appropriate\",\n",
        "# \"approximately\", \"ar\", \"are\", \"aren\", \"arent\", \"aren't\", \"arise\", \"around\", \"as\", \"a's\", \"aside\", \"ask\", \"asking\",\n",
        "# \"associated\", \"at\", \"au\", \"auth\", \"av\", \"available\", \"aw\", \"away\", \"awfully\", \"ax\", \"ay\", \"az\", \"b\", \"b1\", \"b2\", \"b3\",\n",
        "# \"ba\", \"back\", \"bc\", \"bd\", \"be\", \"became\", \"because\", \"become\", \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\",\n",
        "# \"begin\", \"beginning\", \"beginnings\", \"begins\", \"behind\", \"being\", \"believe\", \"below\", \"beside\", \"besides\", \"best\",\n",
        "# \"better\", \"between\", \"beyond\", \"bi\", \"bill\", \"biol\", \"bj\", \"bk\", \"bl\", \"bn\", \"both\", \"bottom\", \"bp\", \"br\", \"brief\",\n",
        "# \"briefly\", \"bs\", \"bt\", \"bu\", \"but\", \"bx\", \"by\", \"c\", \"c1\", \"c2\", \"c3\", \"ca\", \"call\", \"came\", \"can\", \"cannot\", \"cant\",\n",
        "# \"can't\", \"cause\", \"causes\", \"cc\", \"cd\", \"ce\", \"certain\", \"certainly\", \"cf\", \"cg\", \"ch\", \"changes\", \"ci\", \"cit\", \"cj\",\n",
        "# \"cl\", \"clearly\", \"cm\", \"c'mon\", \"cn\", \"co\", \"com\", \"come\", \"comes\", \"con\", \"concerning\", \"consequently\", \"consider\",\n",
        "# \"considering\", \"contain\", \"containing\", \"contains\", \"corresponding\", \"could\", \"couldn\", \"couldnt\", \"couldn't\", \"course\",\n",
        "# \"cp\", \"cq\", \"cr\", \"cry\", \"cs\", \"c's\", \"ct\", \"cu\", \"currently\", \"cv\", \"cx\", \"cy\", \"cz\", \"d\", \"d2\", \"da\", \"date\", \"dc\",\n",
        "# \"dd\", \"de\", \"definitely\", \"describe\", \"described\", \"despite\", \"detail\", \"df\", \"di\", \"did\", \"didn\", \"didn't\",\n",
        "# \"different\", \"dj\", \"dk\", \"dl\", \"do\", \"does\", \"doesn\", \"doesn't\", \"doing\", \"don\", \"done\", \"don't\", \"down\", \"downwards\",\n",
        "# \"dp\", \"dr\", \"ds\", \"dt\", \"du\", \"due\", \"during\", \"dx\", \"dy\", \"e\", \"e2\", \"e3\", \"ea\", \"each\", \"ec\", \"ed\", \"edu\", \"ee\", \"ef\",\n",
        "# \"effect\", \"eg\", \"ei\", \"eight\", \"eighty\", \"either\", \"ej\", \"el\", \"eleven\", \"else\", \"elsewhere\", \"em\", \"empty\", \"en\",\n",
        "# \"end\", \"ending\", \"enough\", \"entirely\", \"eo\", \"ep\", \"eq\", \"er\", \"es\", \"especially\", \"est\", \"et\", \"et-al\", \"etc\", \"eu\",\n",
        "# \"ev\", \"even\", \"ever\", \"every\", \"everybody\", \"everyone\", \"everything\", \"everywhere\", \"ex\", \"exactly\", \"example\",\n",
        "# \"except\", \"ey\", \"f\", \"f2\", \"fa\", \"far\", \"fc\", \"few\", \"ff\", \"fi\", \"fifteen\", \"fifth\", \"fify\", \"fill\", \"find\", \"fire\",\n",
        "# \"first\", \"five\", \"fix\", \"fj\", \"fl\", \"fn\", \"fo\", \"followed\", \"following\", \"follows\", \"for\", \"former\", \"formerly\",\n",
        "# \"forth\", \"forty\", \"found\", \"four\", \"fr\", \"from\", \"front\", \"fs\", \"ft\", \"fu\", \"full\", \"further\", \"furthermore\", \"fy\", \"g\",\n",
        "# \"ga\", \"gave\", \"ge\", \"get\", \"gets\", \"getting\", \"gi\", \"give\", \"given\", \"gives\", \"giving\", \"gj\", \"gl\", \"go\", \"goes\",\n",
        "# \"going\", \"gone\", \"got\", \"gotten\", \"gr\", \"greetings\", \"gs\", \"gy\", \"h\", \"h2\", \"h3\", \"had\", \"hadn\", \"hadn't\", \"happens\",\n",
        "# \"hardly\", \"has\", \"hasn\", \"hasnt\", \"hasn't\", \"have\", \"haven\", \"haven't\", \"having\", \"he\", \"hed\", \"he'd\", \"he'll\", \"hello\",\n",
        "# \"help\", \"hence\", \"her\", \"here\", \"hereafter\", \"hereby\", \"herein\", \"heres\", \"here's\", \"hereupon\", \"hers\", \"herself\",\n",
        "# \"hes\", \"he's\", \"hh\", \"hi\", \"hid\", \"him\", \"himself\", \"his\", \"hither\", \"hj\", \"ho\", \"home\", \"hopefully\", \"how\", \"howbeit\",\n",
        "# \"however\", \"how's\", \"hr\", \"hs\", \"http\", \"hu\", \"hundred\", \"hy\", \"i\", \"i2\", \"i3\", \"i4\", \"i6\", \"i7\", \"i8\", \"ia\", \"ib\",\n",
        "# \"ibid\", \"ic\", \"id\", \"i'd\", \"ie\", \"if\", \"ig\", \"ignored\", \"ih\", \"ii\", \"ij\", \"il\", \"i'll\", \"im\", \"i'm\", \"immediate\",\n",
        "# \"immediately\", \"importance\", \"important\", \"in\", \"inasmuch\", \"inc\", \"indeed\", \"index\", \"indicate\", \"indicated\",\n",
        "# \"indicates\", \"information\", \"inner\", \"insofar\", \"instead\", \"interest\", \"into\", \"invention\", \"inward\", \"io\", \"ip\", \"iq\",\n",
        "# \"ir\", \"is\", \"isn\", \"isn't\", \"it\", \"itd\", \"it'd\", \"it'll\", \"its\", \"it's\", \"itself\", \"iv\", \"i've\", \"ix\", \"iy\", \"iz\", \"j\",\n",
        "# \"jj\", \"jr\", \"js\", \"jt\", \"ju\", \"just\", \"k\", \"ke\", \"keep\", \"keeps\", \"kept\", \"kg\", \"kj\", \"km\", \"know\", \"known\", \"knows\",\n",
        "# \"ko\", \"l\", \"l2\", \"la\", \"largely\", \"last\", \"lately\", \"later\", \"latter\", \"latterly\", \"lb\", \"lc\", \"le\", \"least\", \"les\",\n",
        "# \"less\", \"lest\", \"let\", \"lets\", \"let's\", \"lf\", \"like\", \"liked\", \"likely\", \"line\", \"little\", \"lj\", \"ll\", \"ll\", \"ln\", \"lo\",\n",
        "# \"look\", \"looking\", \"looks\", \"los\", \"lr\", \"ls\", \"lt\", \"ltd\", \"m\", \"m2\", \"ma\", \"made\", \"mainly\", \"make\", \"makes\", \"many\",\n",
        "# \"may\", \"maybe\", \"me\", \"mean\", \"means\", \"meantime\", \"meanwhile\", \"merely\", \"mg\", \"might\", \"mightn\", \"mightn't\", \"mill\",\n",
        "# \"million\", \"mine\", \"miss\", \"ml\", \"mn\", \"mo\", \"more\", \"moreover\", \"most\", \"mostly\", \"move\", \"mr\", \"mrs\", \"ms\", \"mt\",\n",
        "# \"mu\", \"much\", \"mug\", \"must\", \"mustn\", \"mustn't\", \"my\", \"myself\", \"n\", \"n2\", \"na\", \"name\", \"namely\", \"nay\", \"nc\", \"nd\",\n",
        "# \"ne\", \"near\", \"nearly\", \"necessarily\", \"necessary\", \"need\", \"needn\", \"needn't\", \"needs\", \"neither\", \"never\",\n",
        "# \"nevertheless\", \"new\", \"next\", \"ng\", \"ni\", \"nine\", \"ninety\", \"nj\", \"nl\", \"nn\", \"no\", \"nobody\", \"non\", \"none\",\n",
        "# \"nonetheless\", \"noone\", \"nor\", \"normally\", \"nos\", \"not\", \"noted\", \"nothing\", \"novel\", \"now\", \"nowhere\", \"nr\", \"ns\",\n",
        "# \"nt\", \"ny\", \"o\", \"oa\", \"ob\", \"obtain\", \"obtained\", \"obviously\", \"oc\", \"od\", \"of\", \"off\", \"often\", \"og\", \"oh\", \"oi\",\n",
        "# \"oj\", \"ok\", \"okay\", \"ol\", \"old\", \"om\", \"omitted\", \"on\", \"once\", \"one\", \"ones\", \"only\", \"onto\", \"oo\", \"op\", \"oq\", \"or\",\n",
        "# \"ord\", \"os\", \"ot\", \"other\", \"others\", \"otherwise\", \"ou\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"outside\", \"over\",\n",
        "# \"overall\", \"ow\", \"owing\", \"own\", \"ox\", \"oz\", \"p\", \"p1\", \"p2\", \"p3\", \"page\", \"pagecount\", \"pages\", \"par\", \"part\",\n",
        "# \"particular\", \"particularly\", \"pas\", \"past\", \"pc\", \"pd\", \"pe\", \"per\", \"perhaps\", \"pf\", \"ph\", \"pi\", \"pj\", \"pk\", \"pl\",\n",
        "# \"placed\", \"please\", \"plus\", \"pm\", \"pn\", \"po\", \"poorly\", \"possible\", \"possibly\", \"potentially\", \"pp\", \"pq\", \"pr\",\n",
        "# \"predominantly\", \"present\", \"presumably\", \"previously\", \"primarily\", \"probably\", \"promptly\", \"proud\", \"provides\", \"ps\",\n",
        "# \"pt\", \"pu\", \"put\", \"py\", \"q\", \"qj\", \"qu\", \"que\", \"quickly\", \"quite\", \"qv\", \"r\", \"r2\", \"ra\", \"ran\", \"rather\", \"rc\", \"rd\",\n",
        "# \"re\", \"readily\", \"really\", \"reasonably\", \"recent\", \"recently\", \"ref\", \"refs\", \"regarding\", \"regardless\", \"regards\",\n",
        "# \"related\", \"relatively\", \"research\", \"research-articl\", \"respectively\", \"resulted\", \"resulting\", \"results\", \"rf\", \"rh\",\n",
        "# \"ri\", \"right\", \"rj\", \"rl\", \"rm\", \"rn\", \"ro\", \"rq\", \"rr\", \"rs\", \"rt\", \"ru\", \"run\", \"rv\", \"ry\", \"s\", \"s2\", \"sa\", \"said\",\n",
        "# \"same\", \"saw\", \"say\", \"saying\", \"says\", \"sc\", \"sd\", \"se\", \"sec\", \"second\", \"secondly\", \"section\", \"see\", \"seeing\",\n",
        "# \"seem\", \"seemed\", \"seeming\", \"seems\", \"seen\", \"self\", \"selves\", \"sensible\", \"sent\", \"serious\", \"seriously\", \"seven\",\n",
        "# \"several\", \"sf\", \"shall\", \"shan\", \"shan't\", \"she\", \"shed\", \"she'd\", \"she'll\", \"shes\", \"she's\", \"should\", \"shouldn\",\n",
        "# \"shouldn't\", \"should've\", \"show\", \"showed\", \"shown\", \"showns\", \"shows\", \"si\", \"side\", \"significant\", \"significantly\",\n",
        "# \"similar\", \"similarly\", \"since\", \"sincere\", \"six\", \"sixty\", \"sj\", \"sl\", \"slightly\", \"sm\", \"sn\", \"so\", \"some\",\n",
        "# \"somebody\", \"somehow\", \"someone\", \"somethan\", \"something\", \"sometime\", \"sometimes\", \"somewhat\", \"somewhere\", \"soon\",\n",
        "# \"sorry\", \"sp\", \"specifically\", \"specified\", \"specify\", \"specifying\", \"sq\", \"sr\", \"ss\", \"st\", \"still\", \"stop\",\n",
        "# \"strongly\", \"sub\", \"substantially\", \"successfully\", \"such\", \"sufficiently\", \"suggest\", \"sup\", \"sure\", \"sy\", \"system\",\n",
        "# \"sz\", \"t\", \"t1\", \"t2\", \"t3\", \"take\", \"taken\", \"taking\", \"tb\", \"tc\", \"td\", \"te\", \"tell\", \"ten\", \"tends\", \"tf\", \"th\",\n",
        "# \"than\", \"thank\", \"thanks\", \"thanx\", \"that\", \"that'll\", \"thats\", \"that's\", \"that've\", \"the\", \"their\", \"theirs\", \"them\",\n",
        "# \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\", \"thered\", \"therefore\", \"therein\", \"there'll\",\n",
        "# \"thereof\", \"therere\", \"theres\", \"there's\", \"thereto\", \"thereupon\", \"there've\", \"these\", \"they\", \"theyd\", \"they'd\",\n",
        "# \"they'll\", \"theyre\", \"they're\", \"they've\", \"thickv\", \"thin\", \"think\", \"third\", \"this\", \"thorough\", \"thoroughly\",\n",
        "# \"those\", \"thou\", \"though\", \"thoughh\", \"thousand\", \"three\", \"throug\", \"through\", \"throughout\", \"thru\", \"thus\", \"ti\",\n",
        "# \"til\", \"tip\", \"tj\", \"tl\", \"tm\", \"tn\", \"to\", \"together\", \"too\", \"took\", \"top\", \"toward\", \"towards\", \"tp\", \"tq\", \"tr\",\n",
        "# \"tried\", \"tries\", \"truly\", \"try\", \"trying\", \"ts\", \"t's\", \"tt\", \"tv\", \"twelve\", \"twenty\", \"twice\", \"two\", \"tx\", \"u\",\n",
        "# \"u201d\", \"ue\", \"ui\", \"uj\", \"uk\", \"um\", \"un\", \"under\", \"unfortunately\", \"unless\", \"unlike\", \"unlikely\", \"until\", \"unto\",\n",
        "# \"uo\", \"up\", \"upon\", \"ups\", \"ur\", \"us\", \"use\", \"used\", \"useful\", \"usefully\", \"usefulness\", \"uses\", \"using\", \"usually\",\n",
        "# \"ut\", \"v\", \"va\", \"value\", \"various\", \"vd\", \"ve\", \"ve\", \"very\", \"via\", \"viz\", \"vj\", \"vo\", \"vol\", \"vols\", \"volumtype\",\n",
        "# \"vq\", \"vs\", \"vt\", \"vu\", \"w\", \"wa\", \"want\", \"wants\", \"was\", \"wasn\", \"wasnt\", \"wasn't\", \"way\", \"we\", \"wed\", \"we'd\",\n",
        "# \"welcome\", \"well\", \"we'll\", \"well-b\", \"went\", \"were\", \"we're\", \"weren\", \"werent\", \"weren't\", \"we've\", \"what\",\n",
        "# \"whatever\", \"what'll\", \"whats\", \"what's\", \"when\", \"whence\", \"whenever\", \"when's\", \"where\", \"whereafter\", \"whereas\",\n",
        "# \"whereby\", \"wherein\", \"wheres\", \"where's\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whim\", \"whither\",\n",
        "# \"who\", \"whod\", \"whoever\", \"whole\", \"who'll\", \"whom\", \"whomever\", \"whos\", \"who's\", \"whose\", \"why\", \"why's\", \"wi\",\n",
        "# \"widely\", \"will\", \"willing\", \"wish\", \"with\", \"within\", \"without\", \"wo\", \"won\", \"wonder\", \"wont\", \"won't\", \"words\",\n",
        "# \"world\", \"would\", \"wouldn\", \"wouldnt\", \"wouldn't\", \"www\", \"x\", \"x1\", \"x2\", \"x3\", \"xf\", \"xi\", \"xj\", \"xk\", \"xl\", \"xn\",\n",
        "# \"xo\", \"xs\", \"xt\", \"xv\", \"xx\", \"y\", \"y2\", \"yes\", \"yet\", \"yj\", \"yl\", \"you\", \"youd\", \"you'd\", \"you'll\", \"your\", \"youre\",\n",
        "# \"you're\", \"yours\", \"yourself\", \"yourselves\", \"you've\", \"yr\", \"ys\", \"yt\", \"z\", \"zero\", \"zi\", \"zz\",'A', 'AA', 'AB', 'AC',\n",
        "# 'AD', 'AE', 'AF', 'AG', 'AH', 'AI', 'AJ', 'AK', 'AL', 'AM', 'AN', 'AP', 'AR', 'AS', 'AT', 'AU','AV', 'AW', 'AX', 'AY',\n",
        "# 'AZ', 'Aa', 'Ab', 'Ac', 'Ad', 'Ae', 'Af', 'Ah', 'Ai', 'Aj', 'Al', 'Am', 'An', 'Ap', 'Ar', 'As', 'At', 'Au', 'Av', 'Aw',\n",
        "# 'Ax', 'Ay', 'Az', 'B', 'BA', 'BB', 'BC', 'BD', 'BE', 'BF', 'BG', 'BH', 'BI', 'BJ', 'BK', 'BL', 'BM', 'BO', 'BP', 'BQ',\n",
        "# 'BR', 'BS', 'BT', 'BU', 'BV', 'BW', 'BX', 'BY', 'BZ', 'Ba', 'Bc', 'Be', 'Bf', 'Bg', 'Bi', 'Bj', 'Bk', 'Bl', 'Bo', 'Br',\n",
        "# 'Bs', 'Bt', 'Bu', 'By', 'Bì', 'Bò', 'C', 'CA', 'CB', 'CC', 'CD', 'CE', 'CF', 'CG', 'CH', 'CI', 'CJ', 'CK', 'CL', 'CM',\n",
        "# 'CN', 'CO', 'CP', 'CR', 'CS', 'CT', 'CV', 'CW', 'CZ', 'Ca', 'Cc', 'Cd', 'Ce', 'Ch', 'Ci', 'Cj', 'Ck', 'Cm', 'Co', 'Cs',\n",
        "# 'Ct', 'Cu', 'Cv', 'Cy', 'Cz', 'Cá', 'D', 'DA', 'DB', 'DC', 'DD', 'DE', 'DF', 'DG', 'DH', 'DI', 'DJ', 'DK', 'DL', 'DM',\n",
        "# 'DN', 'DO', 'DP', 'DQ', 'DR', 'DS', 'DT', 'DU', 'DV', 'DW', 'DX', 'DY', 'DZ', 'Da', 'Db', 'Dc', 'Dd', 'De', 'Dh', 'Di',\n",
        "# 'Dj', 'Dl', 'Dn', 'Do', 'Dr', 'Ds', 'Dt', 'Du', 'Dw', 'Dx', 'Dz', 'Dà', 'E', 'EA', 'EB', 'EC', 'ED', 'EE', 'EF', 'EG',\n",
        "# 'EH', 'EI', 'EJ', 'EK', 'EL', 'EM', 'EN', 'EO', 'EP', 'EQ', 'ER', 'ES', 'ET', 'EU', 'EV', 'EW', 'EX', 'EY', 'EZ', 'Ea',\n",
        "# 'Ec', 'Ed', 'Ee', 'Eg', 'Eh', 'Ei', 'Ej', 'El', 'Em', 'En', 'Eo', 'Ep', 'Er', 'Es', 'Et', 'Eu', 'Ev', 'Ew', 'Ex', 'Ey',\n",
        "# 'Ez', 'F', 'FA', 'FB', 'FC', 'FD', 'FE', 'FF', 'FG', 'FH', 'FI', 'FJ', 'FK', 'FL', 'FM', 'FN', 'FO', 'FP', 'FR', 'FS',\n",
        "# 'FT', 'FU', 'FW', 'FX', 'Fa', 'Fb', 'Fe', 'Fi', 'Fk', 'Fl', 'Fm', 'Fn', 'Fo', 'Fr', 'Fs', 'Ft', 'Fu', 'G', 'GA', 'GB',\n",
        "# 'GC', 'GD', 'GE', 'GF', 'GG', 'GH', 'GI', 'GJ', 'GK', 'GL', 'GM', 'GN', 'GO', 'GP', 'GQ', 'GR', 'GS', 'GT', 'GV', 'GW',\n",
        "# 'Ga', 'Gd', 'Ge', 'Gf', 'Gi', 'Gj', 'Gm', 'Go', 'Gp', 'Gr', 'Gs', 'Gu', 'Gy', 'H', 'HA', 'HB', 'HC', 'HD','HE', 'HF', 'HG', 'HH', 'HI', 'HJ', 'HK', 'HM', HO', 'HP', 'HQ', 'HR', 'HS', 'HT', 'HV', 'HW', 'HX', 'Ha', 'Hb', 'He',\n",
        "# 'Hh', 'Hi', 'Hj', 'Hk', 'Hm', 'Ho', 'Hr', 'Hs', 'Hu', 'Hv', 'Hw', 'Hy', 'Hà', 'Hù', 'I', 'IA', 'IB', 'IC', 'ID', 'IE',\n",
        "# 'IF', 'IG', 'II', 'IL', 'IM', 'IN', 'IO', 'IP', 'IQ', 'IS', 'IT', 'IU', 'IV', 'IW', 'IX', 'IZ', 'Ic', 'Id', 'Ie', 'If',\n",
        "# 'Ig', 'Ii', 'Ik', 'Il', 'Im', 'In', 'Io', 'Ip', 'Ir', 'Is', 'It', 'Iv', 'Iw', 'Ix', 'Iz', 'J', 'JA', 'JB', 'JC', 'JD',\n",
        "# 'JF', 'JG', 'JH', 'JI', 'JJ', 'JK', 'JL', 'JM', 'JO', 'JP', 'JQ', 'JR', 'JS', 'JT', 'JV', 'JW', 'JY', 'JZ', 'Ja', Jc',\n",
        "# 'Jd', 'Je', 'Ji', 'Jk', 'Jo', 'Jp', 'Jr', 'Js', 'Jt', 'Ju', 'Já', 'K', 'KA', 'KB', 'KC', 'KD', 'KF', 'KG', 'KH', 'KJ',\n",
        "# 'KK', 'KL', 'KM', 'KN', 'KO', 'KP', 'KR', 'KS', 'KT', 'KU', 'KW', 'KY', 'Ka', 'Ke', 'Kg', 'Ki', 'Kj', 'Ko', 'Ks', 'Ku',\n",
        "# 'Ky', 'KÀ', 'KÁ', 'Kà', 'Ká', 'Kā', 'L', 'LA', 'LB', 'LC', 'LD', 'LE', 'LF', 'LG', 'LH', 'LI', 'LJ', 'LK', 'LL', 'LM',\n",
        "# 'LN', 'LO', 'LP', 'LQ', 'LR', 'LS', 'LT', 'LU', 'LV', 'LX', 'LY', 'LZ', 'La', 'Lb', 'Le', 'Lg', 'Li', 'Ll', 'Lm', 'Ln',\n",
        "# 'Lo', 'Ls', 'Lt', 'Lu', 'Lv', 'Ly', 'Là', 'Lá', 'Lé', 'M', 'MA', 'MB', 'MC', 'MD', 'ME', 'MF', 'MG', 'MH', 'MI', 'MJ',\n",
        "# 'MK', 'ML', 'MM', 'MN', 'MO', 'MP', 'MR', 'MS', 'MT', 'MU', 'MV', 'MW', 'MX', 'MY', 'MZ', 'Ma', 'Mb', 'Mc', 'Md', 'Me',\n",
        "# 'Mg', 'Mh', 'Mi', 'Mj', 'Mm', 'Mn', 'Mo', 'Mp', 'Mr', 'Ms', 'Mt', 'Mu', 'Mw', 'Mx', 'My', 'Mì', 'N', 'NA', 'NB', 'NC',\n",
        "# 'ND', 'NE', 'NF', 'NG', 'NH', 'NI', 'NJ', 'NL', 'NM', 'NN', 'NO', 'NP', 'NS', 'NT', 'NU', 'NV', 'NW', 'NY', 'NZ', 'Na',\n",
        "# 'Nd', 'Ne', 'Ng', 'Nh', 'Ni', 'Nj', 'Nm', 'No', 'Np', 'Ns', 'Nu', 'Nv', 'Ny', 'O', 'OB', 'OC', 'OD', 'OE', 'OF', 'OG',\n",
        "# 'OH', 'OI', 'OJ', 'OK', 'OL', 'OM', 'ON', 'OO', 'OP', 'OR', 'OS', 'OT', 'OU', 'OV', 'OW', 'OX', 'OY', 'OZ', 'Ob', 'Oc',\n",
        "# 'Od', 'Of', 'Og', 'Oh', 'Oi', 'Oj', 'Ok', 'Ol', 'Om', 'On', 'Oo', 'Op', 'Or', 'Os', 'Ot', 'Ou', 'Ow', 'Ox', 'Oy', 'Oz',\n",
        "# 'Oí', 'P', 'PA', 'PB', 'PC', 'PD', 'PE', 'PF', 'PG', 'PH', 'PI', 'PJ', 'PK', 'PL', 'PM', 'PN', 'PO', 'PP', 'PR', 'PS',\n",
        "# 'PT', 'PU', 'PV', 'PW', 'PX', 'Pa', 'Pb', 'Pc', 'Pd', 'Pe', 'Pf', 'Ph', 'Pi', 'Pj', 'Pm', 'Po', 'Pr', 'Ps', 'Pt', 'Pu',\n",
        "# 'Pà', 'På', 'Pó', 'Q', 'QA', 'QB', 'QC', 'QF', 'QI', 'QK', 'QQ', 'QR', 'QT', 'Qc', 'Qe', 'Qi', 'Qs', 'Qu', 'R', 'RA',\n",
        "# 'RB', 'RC', 'RD', 'RE', 'RF', 'RH', 'RI', 'RJ', 'RK', 'RL', 'RM', 'RN', 'RO', 'RP', 'RR', 'RS', 'RT', 'RU', 'RV', 'RW',\n",
        "# 'RX', 'Ra', 'Rd', 'Re', 'Ri', 'Rj', 'Rm', 'Ro', 'Rs', 'Rt', 'Ru', 'Rv', 'Rx', 'Ry', 'Rá', 'Rí', 'S', 'SA', 'SB', 'SC',\n",
        "# 'SD', 'SE', 'SF', 'SG', 'SH', 'SI', 'SJ', 'SK', 'SL', 'SM', 'SN', 'SO', 'SP', 'SR', 'SS', 'ST', 'SU', 'SV', 'SW', 'SX',\n",
        "# 'SY', 'Sa', 'Sc', 'Sd', 'Se', 'Sf', 'Sh', 'Si', 'Sk', 'Sm', 'Sn', 'So', 'Sp', 'Sq', 'Sr', 'Ss', 'St', 'Su', 'Sw', 'Sy',\n",
        "# 'SÍ', 'Så', 'Sí', 'Só', 'Sô', 'T', 'TA', 'TB', 'TC', 'TD', 'TE', 'TF', 'TG', 'TH', 'TI', 'TJ', 'TK', 'TL', 'TM', 'TN',\n",
        "# 'TO', 'TP', 'TR', 'TS', 'TT', 'TU', 'TV', 'TW', 'TX', 'TY', 'TZ', 'Ta', 'Te', 'Tf', 'Th', 'Ti', 'Tj', 'Tk', 'Tl', 'Tm',\n",
        "# 'Tn', 'To', 'Tp', 'Tr', 'Ts', 'Tt', 'Tu', 'Tv', 'Tx', 'Ty', 'Tz', 'Tô', 'Tŷ', 'U', 'UA', 'UC', 'UG', 'UH', 'UI', 'UJ',\n",
        "# 'UK', 'UL', 'UM', 'UN', 'UO', 'UP', 'UR', 'US', 'UT', 'UV', 'UW', 'Ua', 'Ud', 'Ug', 'Uh', 'Uk', 'Um', 'Un', 'Up', 'Ur',\n",
        "# 'Us', 'Ut', 'Uw', 'Uy', 'V', 'VA', 'VB', 'VC', 'VD', 'VE', 'VG', 'VH', 'VI', 'VJ', 'VK', 'VM', 'VN', 'VP', 'VR', 'VS',\n",
        "# 'VT', 'VU', 'VV', 'VW', 'VX', 'VZ', 'Va', 'Ve', 'Vi', 'Vm', 'Vn', 'Vs', 'Vt', 'Vu', 'Vy', 'Và', 'Vá', 'W', 'WA', 'WB',\n",
        "# 'WC', 'WD', 'WE', 'WF', 'WG', 'WI', 'WJ', 'WK', 'WM', 'WO', 'WP', 'WR', 'WS', 'WT', 'WU', 'WV', 'WW', 'WY', 'Wa', 'We',\n",
        "# 'Wi', 'Wn', 'Wo', 'Wr', 'Ws', 'Wu', 'Ww', 'X', 'XD', 'XI', 'XL', 'XM', 'XO', 'XP', 'XR', 'XS', 'XT', 'XV', 'XX', 'XY',\n",
        "# 'Xa', 'Xe', 'Xi', 'Xk', 'Xl', 'Xo', 'Xs', 'Xu', 'Xx', 'Y', 'YA', 'YB', 'YC', 'YD', 'YE', 'YG', 'YH', 'YI', 'YK', 'YL',\n",
        "# 'YM', 'YO', 'YS', 'YT', 'YU', 'YY', 'Ya', 'Yb', 'Ye', 'Yi', 'Yk', 'Yo', 'Yr', 'Ys', 'Yu', 'Z', 'ZA', 'ZB', 'ZD', 'ZE',\n",
        "# 'ZI', 'ZK', 'ZZ', 'Za', 'Ze', 'Zi', 'Zo', 'Zu', 'Zy', 'aD', 'aI', 'aM', 'aS', 'aT', 'aa', 'aaa', 'aaaa', 'aaaaa',\n",
        "# 'aaaaaa', 'aaaaaaaaa', 'aaaaaaaaaaaaaaaaaa', 'aaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaand', 'aaaaaaaaaaaaaauthent',\n",
        "# 'aaaaaaaaaaaaaawwwwwwwwwwwwwwwwwwwwww', 'aaaaaaaaaaaah', 'aaaaaaaaaaaahhhhhh', 'aaaaaaaaaaah', 'aaaaaaaaaaand',\n",
        "# 'aaaaaaaaaand', 'aaaaaaaaawesom', 'aaaaaaaaawww', 'aaaaaaaah', 'aaaaaaaahhhhhhhhh', 'aaaaaaaamaz', 'aaaaaaaand',\n",
        "# 'aaaaaaaargh','x']\n",
        "# las_vegas_data['tokenized_text']=las_vegas_data['tokenized_text'].apply(lambda x: [item for item in x if item not in word_list])\n",
        "# print(\"Time to load data: {} seconds\".format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d2c_EqsNyQRD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import os\n",
        "# os.environ[\"GOOGLE_APPLICATION_CREDENTIALS\"]= GOOGLE_APPLICATION_CREDENTIALS='/content/drive/My Drive/yelp-language-detection-ab4d472516d3.json'\n",
        "# from google.cloud import translate\n",
        "# translate_client = translate.Client()\n",
        "# las_vegas_data['language']=las_vegas_data['text'].apply(translate_client.detect_language)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7H3I5jEMw30",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-10EaSz-lBnG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}